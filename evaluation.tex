\cleardoublepage
\chapter{Implementation and Evaluation}
\label{chapter:evaluation}

We implement an evaluation that attempts to explore the overhead of fastPSI's stronger consistency semantics compared to the weakest Read Committed model, and show how fastPSI is able to outperform a protocol implementing the stronger Serialisability consistency model. We also evaluate fastPSI's scalability as more servers and partitions are added to the system, and discuss some of the weak points of the protocol\todonote{rephrase?}.

\section{Implementation}

Our implementation of the fastPSI protocol consists of a client-side library~\citep{pvc-client} and a server~\citep{pvc-server}, written as a plug-in transactional protocol for Antidote~\citep{antidote-db}, a reference platform for evaluating consistency protocols. Both the client library and the server were written in the Erlang programming language, with a total of 6K lines of code. The Antidote platform provides a key-value database, supports both in-memory and disk-based storage, and implements full replication. For simplicity, our implementation only supports in-memory storage, and lacks a replication mechanism. During normal operation, the client-side library communicates with the server using Google's Protocol Buffers~\citep{protobuf}. To enhance network efficiency, client messages are transmitted in periodic batches to the servers.

To validate the results of our evaluation, we also implemented two alternative protocols satisfying the Read Committed and Serialisability consistency models, called \textbf{naiveRC} and \textbf{naiveSER}, respectively. Both were implemented on top of our original implementation and are as efficient as possible. The pseudocode for both implementations can be found in Appendix~\ref{appendix:code}.

Given that fastPSI requires the use of multiple versions per object, it becomes necessary to prevent an unbounded growth of the number of versions in both the $\VersionLog$ database and the number of entries in the per-partition $\CommitLog$. We implemented a simple garbage collection mechanism that maintains a fixed number of versions and that regularly prunes the oldest versions from the state of a partition.\todonote{Bench for this? Tradeoff between number of versions and abort rate. We should talk about Blotters's TTL GC in related work.}

\section{Evaluation}

We evaluate the performance of fastPSI using the Yahoo! Cloud Serving Benchmark~\citep{ycsb}, modified to generate transactional workloads~\citep{ardekani_nmsi, ardekani_gdur}. Our implementation of Read Committed is used as a baseline for comparison in order to show the maximum possible performance. Figure~\ref{fig:workload-types} describes the workloads used. All experiments are run on a cluster consisting of six core machines with 3.80~GHz to 4.70~GHz Intel Xeon processors, 32~GB of RAM, and a single 1~Gbps network port. We partition the cluster in up to three different sites, with four server machines and four client machines per site. Thus, there is no shared memory between clients and servers, as if clients were acting as proxies in the same data centre as servers. Since all our machines are located in the same local network, we artificially add latency between sites using the \texttt{tc} Linux command.

In all our benchmarks, the system is loaded with one million random keys and 256-byte values prior to receiving any operations from the clients. Partitions are distributed uniformly across servers, and keys are mapped to partitions using consistent hashing. Clients are aware of the distribution of keys to partitions and server machines, and thus can directly address the correct server and partition for a specific key. Each client machine spawns multiple concurrent threads that execute transactions and communicate with servers in a closed-loop manner. When transactions read more than object, clients perform those operations serially. For the experiments that involve more than one site, the latency between sites is of 10~ms.

\begin{figure}[h]
\begin{center}
\begin{tabularx}{0.85\linewidth}{ c | c | >{\centering}X | >{\centering}X }
    & \multirow{2}{*}{Key Selection Distribution}
    & \multicolumn{2}{c}{Operations}
\tabularnewline
    & & Read-Only Tran.
    & Update Tran.
\tabularnewline
    \hline
    % A & Zipfian & 4 Reads & 2 Reads, 2 Updates \tabularnewline
    B & Uniform & 4 Reads & 3 Reads, 1 Update \tabularnewline
    C & Uniform & 2 Reads & 1 Read, 1 Update \tabularnewline
    D & Uniform & 3 Reads & 3 Reads, 1 Update \tabularnewline
    E & Uniform & 3 Reads & 3 Reads, 3 Updates \tabularnewline
\end{tabularx}
\end{center}
\caption{Transactional YCSB Workload Types.}
\label{fig:workload-types}
\end{figure}

\subsection{Performance \& Scalability Limits}

\paragraph{Performance.} We begin by investigating the overall performance profile of our implementations by measuring and comparing the throughput and latency of the different protocols as the number of update transactions increases while keeping the number of sites stays constant. For this experiment, we vary the number of concurrent client threads until we reach the saturation point of the system: the maximum throughput it can handle without degrading overall latency in a significant manner. We aim to explore the overall overhead of fastPSI in comparison with naiveRC, and the performance benefits it offers in comparison with naiveSER. Thus, we use Workload C and three sites with 64 partitions uniformly distributed across all sites. Figure~\ref{fig:general_bench} shows our results. We vary the ratio of read-only to update transactions, from 90\%/10\% to 70\%/30\% (left to right), and measure the termination latency of update transactions, i.e., the amount of time spent on the validation process of a transaction. Since the latency is measured in the client, it only reflects the amount of time spent on the \emph{prepare} phase of the commit validation, as the client does not need to wait until the changes of a transaction are committed to the partition state.

\begin{figure}[t]
\vspace{-0.5cm}
\includegraphics[width=\textwidth]{figures/general_bench.pdf}
\vspace{-1cm}
\caption{Comparison of throughput and termination latency of update transactions.}
\label{fig:general_bench}
\end{figure}

Transactions as executed by naiveRC need minimal synchronisation during its commit phase, and no synchronisation at all during read operations. This is translated to a high performance, with the validation of update transactions as the only bottleneck in the system. Thus, as the proportion of update transactions increases, the impact on overall throughput is pronounced, dropping by as much as 20\%.

For both naiveSER and fastPSI, read operations from a transaction $\tx$ wait until $\tx$'s casual dependencies have committed at a particular partition, bounded in the worst case to the maximum latency across sites. In addition, read operations accessing the same partition suffer from synchronisation while fixing a snapshot, as the implementation of $\CommitLog$ is not thread-safe. These two short-comings explain the overall low throughput in comparison with naiveRC. Nevertheless, both implementations exhibit stable performance as the proportion of update transactions increases.

By comparing fastPSI with naiveSER, we observe that the latter implementation is limited by its need to validate every transaction, in comparison with fastPSI, that only validates update transactions. In addition, the relaxed consistency model offered by fastPSI allows it to outperform naiveSER in all cases by approximately 150\%, while showing similar latencies.

\paragraph{Scalability.} To explore the overall scalability of fastPSI, we examine how the maximum performance of each protocol changes as we increase the number of machines in the system. We use Workload B with a fixed ratio of 10\% update transactions. We vary the number of sites, from a single one up to three, while keeping the number of partitions fixed to 64. Figure~\ref{fig:site_bench} shows the overall performance of the protocols.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.5\textwidth]{figures/sites_bench.pdf}
\vspace{-1cm}
\end{center}
\caption{Maximum Throughput of Consistency Models.}
\label{fig:site_bench}
\end{figure}

As before, the performance of naiveRC increases almost in a linear fashion as we add more servers, as explained by its minimum need for synchronisation. Although the scalability of fastPSI is limited by the fixed number of partitions, it benefits moderately from increasing the number of machines, as the overall number of partitions per machine decreases, freeing resources. This is reflected in its performance at three sites being 1.52 times its base throughput at a single site. In contrast, the overall performance of naiveSER stays almost constant as we increase the number of sites, reflecting its need for validating every transaction, which requires increasing synchronisation as the number of machines increases.

As the number of sites increases, so does the difference between naiveSER and fastPSI. Overall, fastPSI manages to outperform naiveSER by a factor of 2.88 with a single site, and by a factor of 3.52 at three sites.

\begin{figure}[h]
\begin{center}
\begin{tabularx}{0.75\linewidth}{ l | >{\centering}p{5cm} | >{\centering}X }
   \textbf{Parameter} &\textbf{Range} &\textbf{Default}
\tabularnewline
    \hline
    Sites & 1--3 & 3
\tabularnewline
    Update Tran. Proportion & 10\%--30\% & 10\%
\tabularnewline
\end{tabularx}
\end{center}
\vspace{-0.5cm}
\caption{Parameter space used in the comparison workload.}
\label{fig:dynamic_parameters}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{figures/dynamic_bench.pdf}
\vspace{-1cm}
\end{center}
\caption{Parameter space exploration to reflect the performance comparison of the protocols. Each experiment varies one parameter while keeping the other fixed at its default value (represented by the grey vertical line). Throughput is shown normalised compared to naiveRC. \todo{Figure out naiveSER line on (b)}}
\label{fig:dynamic_bench}
\end{figure}

\paragraph{Overall comparison.} We've seen how the performance and scalability of the implementations is determined by the proportion of update transactions and the number of sites. To better visualise the relationship between the workload choice and performance, we explore the parameter space described in Figure~\ref{fig:dynamic_parameters} when using Workload B. As in the previous experiment, we keep the number of partitions constant as we increase the number of sites. The results are shown in Figure~\ref{fig:dynamic_bench}, with the throughput depicted normalised compared to the performance of naiveRC.

As shown in Figure~\ref{fig:dynamic_bench}a, fastPSI and naiveSER have different scalability properties. Although both implementations suffer in comparison with naiveRC, fastPSI exhibits much better scalability in comparison with naiveSER. For naiveSER, the need to validate every transaction imposes a performance penalty that increases as we add more sites, and consequently increases the overall latency of the commit phase for every transaction. In contrast, the impact on fastPSI is less severe, as the increased latency only affects the read operations, since the proportion of update transactions is low. Figure~\ref{fig:dynamic_bench}b shows the performance comparison as the proportion of update transactions increases.\todo{Finish this part.}

\subsection{Read Abort Rate Impact}

\todo{Do we want to compare against SER, or just show combinations that make fastPSI abort more or less? I guess we should so fastPSI against SER in some settings to get a sense of how "good" the abort rate is.}

\todo{First, a graph showing how abort rate changes for a certain workload as we increase the proportion of updates. Plot updates on x axis, and abort rate on Y axis. Do we show the proportion of read aborts here?}

\todo{Then, show how certain settings of fastPSI can make read aborts appear/dissappear.}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/abort_rate_bench.pdf}
\vspace{-0.5cm}
\end{center}
\caption{Transaction abort ratio for different consistency models and workloads. Above, overall abort ratio for transactions. Below, ratio of aborts during validation phase (2PC).}
\label{fig:raw_abort_rate}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/psi_read_abort_bench.pdf}
\vspace{-0.5cm}
\end{center}
\caption{Results from exploring fastPSI read abort rate across different workload and deployment scenarios.\todo{Make prettier.}}
\label{fig:fastpsi_abort_rate}
\end{figure}
