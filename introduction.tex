\cleardoublepage
\chapter{Introduction}
\label{chapter:introduction}

%% Victorias's feedback, 2020-04-06
%% The introduction is one of the most important chapters from the POV of the committee.
%% Missing (as explicit subsections?)
%% - Thesis Motivation. Why is this important?
%% - Methodology
%% - Related work
%% - Listing of uni subjects related to the work or that otherwise have helped
%% - Structure of the work (done)

The rise of ubiquitous, globally-available services on the Internet, paired with the ever-increasing amount of data being uploaded to these services, presents two problems: on the one hand, no one service can store all its data in a single computer\todonote{weak, change. Maybe don't mention contrasting requirements, say that redundancy is complex}; on the other hand, users expect that these services operate in a responsive and reliable manner. The solutions are \emph{distribution}, which involves dividing large amounts of data in \emph{shards} or \emph{partitions} among multiple computers; and \emph{replication}, which allows to place identical copies of data in multiple geographical regions. Modern cloud applications use a combination of the two: for example, a social media site might decide to partition its data by user location, placing their data in geographical regions closest to these locations, so that users can access their data without incurring a high latency penalty. In addition, the site might replicate these partitions to other geographical regions to achieve fault-tolerance: no single faulty region can result in data being unavailable.

These approaches, however, add significant complexity to the design, implementation and usage of the databases that usually power cloud applications. The presence of multiple replicas raises the question of how to keep them \emph{consistent}, that is, reflecting an unified version of the data they contain. Traditional relational databases also implement \emph{transactions}, that allow application programmers to reason about their code as a set of \emph{isolated}, atomic operations on shared data. With partitioned databases, transactions might be \emph{distributed}, updating values in multiple partitions, and therefore requiring coordination between them so that changes are applied to the database in an atomic manner.

Traditional approaches to bridge these problems involve relaxing the consistency guarantees that databases offer programmers~\citep{vogels-eventual}. Indeed, the CAP Theorem~\citep{cap-brewer, cap-theorem} proves it is impossible to build applications that continue operating in the presence of network partitions without sacrificing consistency guarantees. However, the degree to which these guarantees can be relaxed offers a trade-off: on the one hand, weak consistency allows to build scalable applications without loss of availability, but proves difficult to reason about given that it allows non-serialisable behaviours called \emph{anomalies}, and forces programmers to deal with inconsistent data at the application level; on the other hand, strengthening consistency guarantees can reduce performance and hurt application availability, while being much easier to reason about.

Until recently, systems that offered weak consistency guarantees did not provide transactions (e.g. Dynamo~\citep{dynamo-amz}). In the recent years, however, a large number of transactional consistency models have been proposed for large-scale databases~\citep{psi-intro, ardekani_nmsi, lloyd_cops, bailis_ramp}. Given the proliferation of different consistency models, it can be hard to choose which one is appropriate for a particular application, as it requires the programmer to think about the possible anomalies that can arise during an execution and about how they can interfere with application logic. Ideally, we'd want to run all applications under strong consistency models, like \emph{serialisability}~\citep{bernstein_concurrency}, as programmers only need to check that application invariants hold as if transactions executed one after the other, without worrying about concurrency. Unfortunately, guaranteeing a serialisable execution in distributed database systems is not possible without requiring global communication, which increases latency and limits availability~\citep{cap-theorem}.

This leaves the programmers with the responsibility of choosing an adequate consistency model for their applications. However, programmers often lack techniques to ensure that a given consistency model is safe to use for particular applications. One way to address this problem is to rely on the notion of \emph{application robustness}~\citep{fekete_ssi, concur_robustness}: an application is robust against a particular consistency model if it behaves in the same way whether using a database providing this model or serialisability. When an application is robust, the programmer can take advantage of the scalability properties of a weak consistency model without paying the price of anomalous behaviours. Previous work has focused on static analysis of applications~\citep{sudhir_static, cise_tool}, which let programmers know which parts of their applications are susceptible to anomalies. In these cases, programmers can selectively run transactions under serialisability: Fekete et al.~\citep{fekete_ssi, fekete_isolation_levels} propose several techniques that allow transactions executing under \emph{snapshot isolation} (SI)~\citep{sql-critique} to exhibit serialisable behaviours, effectively making them equivalent to transactions running under serialisability.

Most recently, Bernardi and Gotsman~\citep{concur_robustness} proposed a way to check the robustness of \emph{parallel snapshot isolation} (PSI)\footnote{Also known as \emph{non-monotonic snapshot isolation}~\citep{ardekani_nmsi}. We discuss this in \textsection\ref{sect:nmsi}.}~\citep{psi-intro}, which relaxes the consistency guarantees of snapshot isolation to allow more efficient implementations for distributed databases. PSI is also the strongest model that is weaker than SI~\citep{concur_framework}, thus it is an obvious candidate to investigate its impact on the correctness of applications. Although there are several implementations that guarantee PSI~\citep{psi-intro, ardekani_nmsi, moniz_blotter}, none of them were implemented with the focus on exploring the relation between PSI and application robustness.

In this work, therefore, we propose fastPSI, an implementation of PSI that allows the programmer to selectively enforce serialisability for transactions through careful grouping of database objects into \emph{entity groups}~\citep{baker_megastore}: transactions accessing objects in the same group execute as if they were running under a system guaranteeing SI (instead of the weaker PSI). Following the techniques proposed by Fekete et al.~\citep{fekete_ssi}, these transactions can be further constrained so that they execute as if running under serialisability.

\pagebreak
% Furthermore, we offer an experimental evaluation of fastPSI, and compare it against alternative implementations of serialisability and read committed, the weakest consistency model that offers atomic transactions. We also discuss some drawbacks of our approach, and ways to minimise their impact.

The contributions of this work are:

\begin{itemize}
    \item A hybrid consistency protocol that allows mixing Snapshot Isolation with Parallel Snapshot Isolation, by relying on entity groups. This protocol allows programmers to combine the scalability of Parallel Snapshot Isolation with the familiarity and intuitiveness of well-known consistency models like Snapshot Isolation and Serialisability.
    \item A comprehensive evaluation of the proposed protocol, and a comparison against alternative implementations of both weak and strong consistency models.
    \item An exposition of the drawbacks and trade-offs of the protocol, and a discussion of how their impact can be minimised.
\end{itemize}

% Our working plan will be as follows. We start with an overview of the different consistency models, as well as the different protocols that satisfy them. We then introduce ways of selectively strengthening consistency guarantees of existing protocols, and propose a new Parallel Snapshot Isolation protocol---fastPSI---that is able to offer stronger guarantees than previous implementations. We further implement such protocol, and perform an evaluation that compares fastPSI against alternative protocols that satisfy alternative consistency models.

The rest of this document is structured as follows. Chapter~\ref{chapter:preliminaries} provides an overview of the most relevant consistency models, as well as basic notions that will be used throughout this document. Chapter~\ref{chapter:protocol} introduces fastPSI, a PSI protocol that allows the programmer to selectively enforce serialisable executions. It also discusses some potential shortcomings of its design. Chapter~\ref{chapter:evaluation} provides a comprehensive evaluation of fastPSI and a comparison against alternative consistency models. Chapter~\ref{chapter:related_work} compares our approach with previous work, and highlights the main differences. Finally, Chapter~\ref{chapter:conclusion} provides our conclusions.
