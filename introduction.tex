\cleardoublepage
\chapter{Introduction}
\label{chapter:introduction}

\section{Motivation}

Modern cloud applications are characterised by being globally available, and users expect to use the services provided by these applications with low latency, and in a reliable manner. To satisfy these requirements, programmers usually resort to distributed databases and storage, that allow to partition application data and place it geographically close to the users that need it. For example, a social media site would place data related to European users on data centers located in the same region, and the same for users in the United States. Under this design, however, those users requesting data from another region would suffer from high latency, as requests travel across different geographical regions. To this end, these data partitions are also replicated across different geographical regions, to ensure both low latency for all kinds of user requests, and fault tolerance, which allows applications to ensure a smooth operation even if an entire region goes offline.

These approaches, however, add significant complexity to the design and implementation of applications and the underlying databases. The presence of multiple replicas raises the question of how to keep them \emph{consistent}, that is, reflecting an unified version of the data they contain. Traditional mechanisms to deal with database consistency prove harder to implement in efficient ways in distributed databases. For example, transactions should satisfy a set of desirable properties, commonly known as ACID: Atomicity, Consistency, Isolation, and Durability. These properties allow programmers to reason about concurrency as a set of isolated, atomic operations, but in geo-distributed scenarios it requires the coordination of multiple replicas in order to apply their updates.

The usual approach to bridge these problems involves relaxing the consistency guarantees that databases offer programmers~\citep{vogels-eventual}. Indeed, the CAP Theorem~\citep{cap-brewer, cap-theorem} proves it is impossible to build applications that continue operating in the presence of network partitions without sacrificing consistency guarantees. However, the degree to which these guarantees can be relaxed offers a trade-off: on the one hand, weak consistency allows to build scalable applications without loss of availability, but proves difficult to reason about given that it allows non-serialisable behaviours called \emph{anomalies}, and forces programmers to deal with inconsistent data at the application level; on the other hand, strengthening consistency guarantees can reduce performance and hurt application availability, while being much easier to reason about.

Until recently, systems that offered weak consistency guarantees did not provide transactions (e.g. Dynamo~\citep{dynamo-amz}). In the recent years, however, a large number of transactional consistency models have been proposed for large-scale databases~\citep{psi-intro, ardekani_nmsi, lloyd_cops, bailis_ramp}. Given the proliferation of different consistency models, it can be hard to choose which one is appropriate for a particular application, as it requires the programmer to think about the possible anomalies that can arise during an execution and about how they can interfere with application logic. Ideally, we'd want to run all applications under strong consistency models, like \emph{serialisability}~\citep{bernstein_concurrency}, as programmers only need to check that application invariants hold as if transactions executed one after the other, without worrying about concurrency. Unfortunately, guaranteeing a serialisable execution in distributed database systems is not possible without requiring global communication, which increases latency and limits availability~\citep{cap-theorem}.

This leaves the programmers with the responsibility of choosing an adequate consistency model for their applications. However, programmers often lack techniques to ensure that a given consistency model is safe to use for particular applications. One way to address this problem is to rely on the notion of \emph{application robustness}~\citep{fekete_ssi, concur_robustness}: an application is robust against a particular consistency model if it behaves in the same way whether using a database providing this model or serialisability. When an application is robust, the programmer can take advantage of the scalability properties of a weak consistency model without paying the price of anomalous behaviours. Previous work has focused on static analysis of applications~\citep{sudhir_static, cise_tool}, which let programmers know which parts of their applications are susceptible to anomalies. In these cases, programmers can selectively run transactions under serialisability: Fekete et al.~\citep{fekete_ssi, fekete_isolation_levels} propose several techniques that allow transactions executing under \emph{snapshot isolation} (SI)~\citep{sql-critique} to exhibit serialisable behaviours, effectively making them equivalent to transactions running under serialisability.

Most recently, Bernardi and Gotsman~\citep{concur_robustness} proposed a way to check the robustness of \emph{parallel snapshot isolation} (PSI)\footnote{Also known as \emph{non-monotonic snapshot isolation}~\citep{ardekani_nmsi}. We discuss this in \textsection\ref{sect:nmsi}.}~\citep{psi-intro}, which relaxes the consistency guarantees of snapshot isolation to allow more efficient implementations for distributed databases. PSI is also the strongest model that is weaker than SI~\citep{concur_framework}, thus it is an obvious candidate to investigate its impact on the correctness of applications. Although there are several implementations that guarantee PSI~\citep{psi-intro, ardekani_nmsi, moniz_blotter}, none of them were implemented with the focus on exploring the relation between PSI and application robustness.

\section{Goals}

Our goal is to help programmers to bridge the gap between weak and strong consistency protocols, without sacrificing application correctness. To that end, we propose fastPSI, an implementation of Parallel Snapshot Isolation that allows to selectively enforce serialisability for transactions through careful grouping of database objects into \emph{entity groups}~\citep{baker_megastore}: transactions accessing objects in the same group execute as if they were running under a system guaranteeing SI (instead of the weaker PSI). Following the techniques proposed by Fekete et al.~\citep{fekete_ssi}, these transactions can be further constrained so that they execute as if running under serialisability.\\

As such, the contributions of this work are:

\begin{itemize}
    \item A hybrid consistency protocol that allows mixing Snapshot Isolation with Parallel Snapshot Isolation, by relying on entity groups. This protocol allows programmers to combine the scalability of Parallel Snapshot Isolation with the familiarity and intuitiveness of well-known consistency models like Snapshot Isolation and serialisability.

    \item A comprehensive evaluation of the proposed protocol, and a comparison against alternative implementations of both weak and strong consistency models.

    \item An exposition of the drawbacks and trade-offs of the protocol, and a discussion of how their impact can be minimised.
\end{itemize}

\section{Work Plan}

The work carried out for this project was divided in the following phases:

\begin{itemize}
    \item Explore previous contributions. It's necessary to get familiarised with existing terminology, as well as previous work and implementations.

    \item Delimit project scope. After having the necessary knowledge to carry out the work, we established the limits and specific contributions of our work. We also establish the hypotheses that our final implementation should validate.

    \item Implementation phase. After settling clear objectives and milestones, we carried out the bulk of the implementation. Throughout this phase, we also interleaved testing and validation of the software, both with unit tests and with model checking techniques.

    \item Benchmark and validation. With the implementation complete, we design representative benchmarks and scenarios to validate the performance of our implementation. We also validate our previous hypotheses, and frame our results in the context of the existing literature.
\end{itemize}

\section{Document Structure}

The rest of this document is structured as follows. Chapter~\ref{chapter:preliminaries} provides an overview of the state of the art with regards to relevant consistency models, as well as basic notions that will be used throughout this document. Chapter~\ref{chapter:protocol} introduces fastPSI, a PSI protocol that allows the programmer to selectively enforce serialisable executions. It also discusses some potential shortcomings of its design. Chapter~\ref{chapter:evaluation} provides a comprehensive evaluation of fastPSI and a comparison against alternative consistency models. Chapter~\ref{chapter:related_work} compares our approach with the previous work outlined in Chapter~\ref{chapter:preliminaries}, and highlights the main differences. Finally, Chapter~\ref{chapter:conclusion} provides our conclusions.

\section{Sources and Repositories}

The relevant sources related to this document can be found online at the following repositories:

\begin{itemize}
    \item The server-side of the fastPSI protocol, to be found at \url{https://github.com/ergl/antidote/tree/pvc}.

    \item The client-side library of the fastPSI protocol, to be found at \url{https://github.com/ergl/pvc/tree/master}.

    \item The software used to benchmark fastPSI, modified with extra features, to be found at \url{https://github.com/ergl/lasp-bench/tree/coord}.

    \item The benchmark data, along with several scripts used to generate the plots on Chapter~\ref{chapter:evaluation}, to be found at \url{https://github.com/ergl/antidote-evaluation}.
\end{itemize}

\section{Related Program Courses}

Several courses during the Computer Science program were of great help to learn the foundations and the necessary skills to carry out the work in this document:

\begin{itemize}
    \item \emph{Databases} and \emph{Advanced Databases}, which taught the foundational concepts of transactions, consistency and concurrency control. In particular, the latter covered the implementation and design of the storage subsystems of databases.

    \item \emph{Networks} and \emph{Advanced Operating Systems and Networks}, which taught the necessary skills to design and implement an efficient client-server protocol on top of TCP.

    \item \emph{Technology and Organisation of Computer Systems}, which taught the necessary cache and distributed memory designs that allowed an increased performance of the fastPSI server implementation.

    \item \emph{Probability and Statistics} and \emph{Evaluation of Computer Systems}, which taught the foundational concepts needed for an effective interpretation of benchmark data, along with theoretical models---such as queueing theory---necessary to tune performance and to design effective performance experiments.
\end{itemize}
