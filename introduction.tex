\cleardoublepage
\chapter{Introduction}
\label{chapter:introduction}

The rise of ubiquitous, globally-available services on the Internet, paired with the ever-increasing amount of data being uploaded to these services, presents two problems: on the one hand, no one service can store all its data in a single computer; on the other hand, users expect that these services operate in a responsive and reliable manner. The solutions are \emph{distribution}, which involves dividing large amounts of data in \emph{shards} or \emph{partitions} among multiple computers; and \emph{replication}, which allows to place identical copies of data in multiple geographical regions. Modern cloud applications use a combination of the two: for example, a social media site might decide to partition its data by user location, placing their data in geographical regions closest to these locations, so that users can access their data without incurring a high latency penalty. In addition, the site might replicate these partitions to other geographical regions to achieve fault-tolerance: no single faulty region can result in data being unavailable.

These approaches, however, add significant complexity to the design, implementation and usage of the databases that usually power cloud applications. The presence of multiple replicas raises the question of how to keep them \emph{consistent}, that is, reflecting an unified version of the data they contain. Traditional relational databases also implement \emph{transactions}, that allow application programmers to reason about their code as a set of \emph{isolated}, atomic operations on shared data. With partitioned databases, transactions might be \emph{distributed}, updating values in multiple partitions, and therefore requiring coordination between them so that changes are applied to the database in an atomic manner.

Traditional approaches to bridge these problems involve relaxing the consistency guarantees that these databases offer the programmers~\citep{vogels-eventual}. Indeed, the CAP Theorem~\citep{cap-brewer, cap-theorem} proves it is impossible to build applications that continue operating in the presence of network partitions without sacrificing consistency guarantees. However, the degree to which these guarantees can be relaxed offers a trade-off: on the one hand, weak consistency guarantees allow to build scalable applications without loss of availability, but prove difficult to reason about given that they allow non-serialisable behaviours called \emph{anomalies}, and force programmers to deal with inconsistent data at the application level; on the other hand, strengthening consistency guarantees can reduce performance and hurt application availability, while being much easier to reason about.

Until recently, systems that offered weak consistency guarantees did not provide transactions (e.g. Dynamo~\citep{dynamo-amz}). In the recent years, however, a large number of transactional consistency models have been proposed for large-scale databases~\citep{psi-intro, ardekani_nmsi, lloyd_cops, bailis_ramp}. Given the proliferation of different consistency models, it can be hard to choose which one is appropriate for a particular application, as it requires the programmer to think about the possible anomalies that can arise during an execution and about how they can interfere with application logic. Ideally, we'd want to run all applications under strong consistency models, like \emph{serialisability}~\citep{bernstein_concurrency}, as programmers only need to check that application invariants hold as if transactions executed one after the other, without worrying about concurrency. Unfortunately, guaranteeing a serialisable execution in distributed database systems is not possible without requiring global communication, which increases latency and limits availability~\citep{cap-theorem}.

This leaves the programmers with the responsibility of choosing an adequate consistency model for their applications. However, programmers often lack techniques to ensure that a given consistency model is safe to use for particular applications. One way to address this problem is to rely on the notion of \emph{application robustness}~\citep{fekete_ssi, fekete_isolation_levels, concur_robustness}: an application is robust against a particular consistency model if it behaves in the same way whether using a database providing this model or serialisability. When an application is robust, the programmer can take advantage of the scalability properties of a weak consistency model without paying the price of anomalous behaviours. Previous work has focused on static analysis of applications~\citep{sudhir_static, cise_tool}, which let programmers know which parts of their applications are susceptible to anomalies. In these cases, programmers can selectively run transactions under serialisability: Fekete et al.~\citep{fekete_ssi} propose several techniques that allow transactions executing under \emph{snapshot isolation} (SI)~\citep{sql-critique} to exhibit serialisable behaviours, effectively making them equivalent to transactions running under serialisability.

Most recently, Bernardi and Gotsman~\citep{concur_robustness} proposed a way to check the robustness of \emph{parallel snapshot isolation} (PSI)\footnote{Also known as \emph{non-monotonic snapshot isolation}~\citep{ardekani_nmsi}. We discuss this in \textsection\ref{sect:nmsi}.}~\citep{psi-intro}, which relaxes the consistency guarantees of snapshot isolation to allow more efficient implementations for distributed databases. PSI is also the strongest model that is weaker than SI~\citep{concur_framework}, thus it is an obvious candidate to investigate its impact on the correctness of applications.

% Mention G-DUR in implementation?
Although there are several implementations that guarantee PSI~\citep{psi-intro, ardekani_nmsi, moniz_blotter}, none of them were implemented with the focus on exploring the relation between PSI and application robustness. To this end, in this work we propose fastPSI, an implementation of PSI that allows the programmer to selectively enforce serialisability for transactions through careful grouping of database objects into \emph{entity groups}: transactions accessing objects in the same group execute as if they were running under a system guaranteeing SI (instead of the weaker PSI). Following the techniques proposed by Fekete et al.~\citep{fekete_ssi}, these transactions can be further modified so that they execute as if running under serialisability. Furthermore, we offer an experimental evaluation of fastPSI, and compare it against alternative implementations of serialisability and read committed, the weakest consistency model that offers atomic transactions. We also discuss some drawbacks of our approach, and ways to minimise their impact.